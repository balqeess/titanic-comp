{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "filepath = \"/Users/balqeesjabri/Downloads/titanic.csv\"\n",
    "titanic_data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Assuming 'preds' contains the predicted labels for the validation set, and 'y_valid' contains the true labels\n",
    "def evaluate_classification_model(y_valid, preds):\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(y_valid, preds)\n",
    "    recall = recall_score(y_valid, preds)\n",
    "    f1 = f1_score(y_valid, preds)\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_valid, preds)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name        891\n",
       "Sex           2\n",
       "Ticket      681\n",
       "Cabin       147\n",
       "Embarked      3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.select_dtypes([\"object\"]).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will drop the name column since this feature does not add any meaningful information and is unique for each sample,\n",
    "# it is unlikely to help your model make accurate predictions.\n",
    "titanic_data = titanic_data.drop(columns=['Name', 'PassengerId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex', 'Ticket', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [colname for colname in titanic_data.columns if titanic_data[colname].dtype == \"object\"]\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare',\n",
       "       'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare',\n",
       "       'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = titanic_data.drop(columns=\"Survived\")\n",
    "y = titanic_data[\"Survived\"]\n",
    "\n",
    "# Split the data into training and encoding sets\n",
    "X_train_full, X_encode_full, y_train, y_encode = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# Choose a set of features to encode and a value for m\n",
    "encoder = MEstimateEncoder(cols=categorical_cols, m=5.0)\n",
    "\n",
    "# Fit the encoder on the encoding set\n",
    "encoder.fit(X_encode_full, y_encode)\n",
    "\n",
    "# Encode the encoding set\n",
    "X_encoded = encoder.transform(X_encode_full, y_encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training set using the already fitted encoder\n",
    "X_train_encoded = encoder.transform(X_train_full, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter categorical cols only\n",
    "categorical_features = [colname for colname in X_train_full.columns if X_train_full[colname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_features = [colname for colname in X_train_full.columns if X_train_full[colname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_features + numerical_features\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_encode_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 549, 1: 342})\n",
      "Resample dataset shape Counter({0: 549, 1: 549})\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "# fit predictor and target variablex_ros, y_ros = ros.fit_resample(x, y)\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "\n",
    "print('Original dataset shape', Counter(y))\n",
    "print('Resample dataset shape', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from xgboost import XGBClassifier\n",
    "# Assuming you have defined your numerical_cols and categorical_cols appropriately\n",
    "\n",
    "# Step 1: Create transformers for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Step 2: Create a ColumnTransformer to apply different transformers to different feature types\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# define model \n",
    "my_model = XGBClassifier(n_estimators=900, learning_rate=0.07)\n",
    "\n",
    "# Step 5: Create the pipeline with TomekLinks undersampling and the model\n",
    "my_pipeline = Pipeline(steps=[\n",
    "  \n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', my_model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line fits the pipeline to the training data,\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7752808988764045\n",
      "Recall: 0.8214285714285714\n",
      "F1-score: 0.7976878612716762\n",
      "Confusion Matrix:\n",
      "[[119  20]\n",
      " [ 15  69]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_classification_model(y_encode, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
