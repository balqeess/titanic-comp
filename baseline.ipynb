{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "filepath = \"/Users/balqeesjabri/Downloads/titanic.csv\"\n",
    "titanic_data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can conclude that Cabin has the most missing values followed by Age and embarked. However, there can still be other ways that missing data can be hiding in the data. For example,object data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate variables into numerical and categorical\n",
    "numerical_vars = titanic_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_vars = titanic_data.select_dtypes(include=['object']).columns\n",
    "# Display the number of numerical and categorical variables\n",
    "print(f\"Number of Numerical variables: {len(numerical_vars)}\")\n",
    "print(f\"Number of Categorical variables: {len(categorical_vars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical variables\n",
    "titanic_data[numerical_vars].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptive statistics of the numerical variables give us the following insights:\n",
    "\n",
    "PassengerId: The average PassengerId is around 446, with a minimum of 1 and a maximum of 891. The standard deviation is approximately 257.35\n",
    "\n",
    "Survived: The average of Survivors is around 0.38.The standard deviation is around 0.4866.\n",
    "\n",
    "Age: The average Age is 29.7, but the standard deviation is 14.5.\n",
    "\n",
    "Fare: The average overall Fare is about 32.2, with a standard deviation of approximately 49.69.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic_data['Survived']  # Add the target variable as a new column\n",
    "\n",
    "# Create the pair plot with 'hue' as the target variable\n",
    "sns.pairplot(titanic_data.drop(columns='PassengerId'), hue='Survived', markers=[\"o\", \"s\"], palette='tab10')\n",
    "\n",
    "# Set the title for the pair plot\n",
    "plt.suptitle(\"Pair Plot with Hue for Target Variable\", y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the reason why categorical values are not showing becuase most plotting libraries, including Seaborn and Matplotlib, require numerical data for creating visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = titanic_data.select_dtypes(include=['object']).drop(columns='Name')\n",
    "# Plot each categorical feature separately\n",
    "\n",
    "# Loop through each categorical feature\n",
    "for feature in categorical_features:\n",
    "    # Get the top N most frequent categories\n",
    "    top_n_categories = categorical_features[feature].value_counts().nlargest(5)\n",
    "\n",
    "\n",
    "    # Plot the bar plot for the top N categories\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    sns.barplot(x=top_n_categories.index, y=top_n_categories.values)\n",
    "    plt.title(f'Top {5} {feature} Categories')\n",
    "    plt.xlabel(f'{feature} Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for missing values in numerical variables\n",
    "num_missing_values = titanic_data[numerical_vars].isnull().sum()\n",
    "num_missing_values = num_missing_values[num_missing_values > 0]\n",
    "# Calculate the percentage of missing values\n",
    "num_missing_percent = num_missing_values / len(titanic_data) * 100\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "num_missing_df = pd.DataFrame({'Missing Values': num_missing_values, 'Percentage': num_missing_percent})\n",
    "num_missing_df.sort_values(by='Percentage', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above lists numerical variables that contain missing values along with the percentage of missing data for each.\n",
    "\n",
    "Age: Approximately 19.86532% of the data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in categorical variables\n",
    "cat_unique_counts = titanic_data[categorical_vars].nunique().sort_values(ascending=False)\n",
    "\n",
    "# Check for missing values in categorical variables\n",
    "cat_missing_values = titanic_data[categorical_vars].isnull().sum()\n",
    "cat_missing_values = cat_missing_values[cat_missing_values > 0]\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "cat_missing_percent = cat_missing_values / len(titanic_data) * 100\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "cat_missing_df = pd.DataFrame({'Unique Values': cat_unique_counts, 'Missing Values': cat_missing_values, 'Percentage': cat_missing_percent})\n",
    "cat_missing_df.sort_values(by='Missing Values', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above lists categorical variables, the count of unique values, missing values, and the percentage of missing data for each.\n",
    "\n",
    "Cabin has a high percentage of missing values while Embarked has a low percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION MATRIX\n",
    "numerical_features = titanic_data.select_dtypes(include=['int64', 'float64']).drop(columns='PassengerId')\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numerical_features.corr()\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation with Target: Pclass and Age\n",
    "Pairwise Correlations: Fare and Pclass, Age and Pclass\n",
    "\n",
    "There is no set value for what counts as “highly correlated”, however a general rule is a correlation of 0.7 (or -0.7). There are no pairs of features with a correlation of 0.7 or higher, so we do not need to consider leaving any features out of our model based on multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have classification problem so we need to check the balance of the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CLASS DISTRIBUTION \n",
    "class_distribution = y.value_counts()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Proportions: Calculate the proportion of samples in each class relative to the total number of samples.\n",
    "# If any class has a significantly smaller proportion, it indicates an imbalanced dataset.\n",
    "class_proportions = y.value_counts(normalize=True)\n",
    "print(class_proportions*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot: Visualize the class distribution using a bar plot. \n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(y)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Target Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this plot indicates that my dataset is highly imbalanced, it is possible that the \"survived\" class has very few samples, leading to the appearance of a single bar with the value zero on the x-axis. In such cases, the bar representing the minority class (survived) might be too small to be visible in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot: Visualize the class distribution using a horizontal bar plot.\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=y)  # Use x parameter instead of y\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Target Class')  # Switch the axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Imbalance Ratio: Calculate the imbalance ratio,\n",
    "# which is the ratio of the size of the majority class to the size of the minority class.\n",
    "\n",
    "imbalance_ratio = class_distribution.max() / class_distribution.min()\n",
    "print(\"Imbalance Ratio:\", imbalance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the ratio is more than one, it suggests the dateset is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There is imbalance between two classes (approx 28% difference).So consider adapting performance metrics like \"recall\", \"precision\" and \"f1 score\" instead of \"accurary\" to assess the ML model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESAMPLING TECHNIQUES TO SOLVE CLASS IMBALANCE \n",
    "# Separating the target that will be 0 and class 1.\n",
    "# survivor_count_0, survivor_count_1 = titanic_data['Survived'].value_counts()\n",
    "\n",
    "# Separate target\n",
    "survivor_0 = titanic_data[titanic_data['Survived'] == 0]\n",
    "survivor_1 = titanic_data[titanic_data['Survived'] == 1]# print the shape of the class\n",
    "print('Survived 0:', survivor_0.shape)\n",
    "print('Survived 1:', survivor_1.shape)\n",
    "\n",
    "# target count \n",
    "survivor_count_0 = len(survivor_0)\n",
    "survivor_count_1 = len(survivor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Under-Sampling: removing some observations of the majority class.\n",
    "# This is done until the majority and minority class is balanced out.\n",
    "# can be a good choice when you have a ton of data -think millions of rows.\n",
    "# But a drawback to undersampling is that we are removing information that may be valuable.\n",
    "\n",
    "\"\"\" This line randomly samples a subset of samples from the majority class (Survived=0) so that the number of samples in survivor_0_under\n",
    "    becomes equal to the number of samples in the minority class (Survived=1).\n",
    "    This is the under-sampling step, which creates a balanced dataset with an equal number of samples from both classes. \"\"\"\n",
    "# Calculate the count of samples in each class\n",
    "\n",
    "\n",
    "survivor_0_under = survivor_0.sample(survivor_count_1)\n",
    "\n",
    "test_under = pd.concat([survivor_0_under, survivor_1], axis=0)\n",
    "\n",
    "print(\"total survive of 1 and0:\",test_under['Survived'].value_counts())# plot the count after under-sampeling\n",
    "test_under['Survived'].value_counts().plot(kind='bar', title='count (target)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Over-Sampling\n",
    "\"\"\" Oversampling can be defined as adding more copies to the minority class.\n",
    "    Oversampling can be a good choice when you don’t have a ton of data to work with.\n",
    "    A con to consider is that it can cause overfitting and poor generalization to your test set. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform over-sampling on the minority class (Survived=1)\n",
    "survivor_1_over = survivor_1.sample(survivor_count_0, replace=True)\n",
    "\n",
    "# Concatenate the over-sampled minority class with the majority class\n",
    "test_over = pd.concat([survivor_1_over, survivor_0], axis=0)\n",
    "\n",
    "# Print the total number of samples for each class after over-sampling\n",
    "print(\"total survivors of 1 and 0:\", test_over['Survived'].value_counts())\n",
    "\n",
    "# Plot the count after over-sampling\n",
    "test_over['Survived'].value_counts().plot(kind='bar', title='count (target)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Balance Data With the Imbalanced-Learn Python Module?\n",
    "A number of more sophisticated resampling techniques have been proposed in the scientific literature.\n",
    "\n",
    "For example, we can cluster the records of the majority class and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\n",
    "X = titanic_data.drop(columns='Survived')\n",
    "y = titanic_data['Survived']\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "\n",
    "print('original dataset shape:', Counter(y))\n",
    "print('Resample dataset shape', Counter(y_rus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Over-Sampling With imblearn\n",
    "One way to fight imbalanced data is to generate new samples in the minority classes. The most naive strategy is to generate new samples by random sampling with the replacement of the currently available samples. The RandomOverSampler offers such a scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X = titanic_data.drop(columns='Survived')\n",
    "y = titanic_data['Survived']\n",
    "# fit predictor and target variablex_ros, y_ros = ros.fit_resample(x, y)\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "\n",
    "print('Original dataset shape', Counter(y))\n",
    "print('Resample dataset shape', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under-Sampling: Tomek Links\n",
    "Tomek links are pairs of very close instances but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks()\n",
    "\n",
    "# fit predictor and target variable\n",
    "X_tl, y_tl = tl.fit_resample(X, y)\n",
    "\n",
    "print('Original dataset shape', Counter(y))\n",
    "print('Resample dataset shape', Counter(y_tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assuming 'test_under' is the balanced DataFrame containing both classes\n",
    "# X = test_over.drop('Survived', axis=1)  # Features (all columns except 'Survived')\n",
    "# y = test_over['Survived']  # Target variable\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_tl, y_tl, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter categorical cols only\n",
    "categorical_cols = [colname for colname in X_train_full.columns if X_train_full[colname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [colname for colname in X_train_full.columns if X_train_full[colname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Assuming you have defined your numerical_cols and categorical_cols appropriately\n",
    "\n",
    "# Step 1: Create transformers for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Step 2: Create a ColumnTransformer to apply different transformers to different feature types\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Define the Model\n",
    "# # Next, we define a random forest model with the familiar RandomForestRegressor class.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=250, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipline \n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "# This line fits the pipeline to the training data,\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Assuming 'preds' contains the predicted labels for the validation set, and 'y_valid' contains the true labels\n",
    "precision = precision_score(y_valid, preds)\n",
    "recall = recall_score(y_valid, preds)\n",
    "f1 = f1_score(y_valid, preds)\n",
    "conf_matrix = confusion_matrix(y_valid, preds)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1 RANDOM UNDERSAMPLING SCORE:\n",
    "Precision: 0.6836734693877551\n",
    "Recall: 0.8701298701298701\n",
    "F1-score: 0.7657142857142857\n",
    "Confusion Matrix:\n",
    "[[63 31]\n",
    " [10 67]]\n",
    "\n",
    "METHOD 2 RANDOM OVERSAMPLING SCCORE:\n",
    "Precision: 0.9315068493150684\n",
    "Recall: 0.918918918918919\n",
    "F1-score: 0.9251700680272108\n",
    "Confusion Matrix:\n",
    "[[117  10]\n",
    " [ 12 136]]\n",
    "\n",
    "METHOD 3 RANDOM UNDERSAMPLING WITH IMBLEARN SCORE:\n",
    "Precision: 0.8023255813953488\n",
    "Recall: 0.8961038961038961\n",
    "F1-score: 0.8466257668711655\n",
    "Confusion Matrix:\n",
    "[[77 17]\n",
    " [ 8 69]]\n",
    "\n",
    "METHOD 4 RANDOM OVERSAMPLING WITH IMBLEARN SCORE:\n",
    "Precision: 0.8407643312101911\n",
    "Recall: 0.9361702127659575\n",
    "F1-score: 0.8859060402684564\n",
    "Confusion Matrix:\n",
    "[[109  25]\n",
    " [  9 132]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic Minority Oversampling Technique (SMOTE)\n",
    "This technique generates synthetic data for the minority class.\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE algorithm works in 4 simple steps:\n",
    "\n",
    "Choose a minority class as the input vector.\n",
    "Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function).\n",
    "Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor.\n",
    "Repeat the steps until the data is balanced.\n",
    "\n",
    "HOWEVER all features must be numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # import library\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# smote = SMOTE()\n",
    "\n",
    "# # fit predictor and target variable\n",
    "# x_smote, y_smote = smote.fit_resample(x, y)\n",
    "\n",
    "# print('Original dataset shape', Counter(y))\n",
    "# print('Resample dataset shape', Counter(y_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NearMiss\n",
    "NearMiss is an under-sampling technique. Instead of resampling the Minority class, using a distance will make the majority class equal to the minority class.\n",
    "\n",
    "HOWEVER all features must be numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# nm = NearMiss()\n",
    "\n",
    "# x_nm, y_nm = nm.fit_resample(x, y)\n",
    "\n",
    "# print('Original dataset shape:', Counter(y))\n",
    "# print('Resample dataset shape:', Counter(y_nm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
